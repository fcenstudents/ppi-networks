
------
Se generó una red aleatoria según el modelo de Erdos-Rényi mediante el
archivo create-erdosrenyi-network.py. Se usó una implementación de 
aquel modelo de NetworkX con valores de parámetros n = 30 (número de nodos), p = 0.20 
(probabilidad de que un nodo se conecte al nodo i ya existente) y directed = False.
Esto dio como resultado una red de 30 nodos, no direccionada y simple. Finalmente, se guardó
la lista de adyacencia correspondiente en el archivo er-n30-p02.txt dentro de la carpeta denominada data.
------

Se generaron 30 embeddings usando deepwalk variando el número de caminos (1, 5, 10, 25, 50 y 100) y la longitud de
los mismos (5, 10, 25, 50 y 100). Se fijó el tamaño de la vantana (window-size) en 3 y la
dimensión del embedding (representation-size) en 2.

Ej. de uso de deepwalk: (por default seed = 0)

deepwalk --format adjlist --input data/er-n30-p02.txt --max-memory-data-size 0 --number-walks 1 
-- representation-size 2 --walk-length 5 --window-size 3 --workers 4 
--output deepwalk-embeddings/w3-x1-y5-er-n30-p02.txt

Los embeddings se guardaban en achivos .txt dentro de la carpeta denominada deepwalk-embeddings.

------

Se realizó un preprocesamiento de los embeddings obtenidos para una adecuada comparación entre sí.
Se llevó a cabo una normalización, ejecutando el archivo normalize-embedding.py, que consistió en
restar por la media y dividir por la desviación estándar en cada dimensión. Los nuevos embeddings
se guardaron en la carpeta embeddings-normalized.

------

A partir de los embeddings normalizados se generó el archivo fig-embeddings.pdf en la carpeta visualizations
mediante figura.py con una única figura que contiene como subplots a los embeddings. Se puede apreciar la
disposición de los nodos a medida que se varían el número de caminos y su longitud.

------

IDEA 1.

En relación a la normalización efectuada sobre los embeddings originales nos preguntamos si exite
algún efecto tras variar los valores de número y longitud de caminos. 

Usando idea-1.py, se genera el archivo idea-1-er-n30-p02.txt con # de caminos, longitud de caminos, xmean,
ymean, xstd e ystd, y tres scatterplots idea-1-mean.pdf, idea-1-std.pdf y idea-1-mean-std.pdf en
la carpeta visualizations. De esta manera, se puede observa la distribución de las medias,
desviaciones estándar y el cociente media/desviación estándar al variar los valores de los parámetros.

------

IDEA 2.

Se quiere ver relación entre los shortest path entre los nodos y la distancia euclideana de los mismos
en los embeddings.

Para ello se realizó lo siguiente:

+ Con shortest-path-nodes.py se calculó los shortest paths entre los nodos del grafo. Se
generó el archivo s-path-nodes.txt con una lista de nodei, nodej, path-lenght.

+ Con distance-embeddings.py se calculó la distancia euclideana entre los nodos en los embeddings
normalizados. Se generaron archivos dist-embedding-nodes.txt con una lista de nodei, nodej y distance
para cada combinación de parámetros.

+ Con idea-2-3.py se generó el archivo idea-2.pdf con una única figura con los scatterplots de
shortest-path vs. distance-embeddings en la carpeta visualizations.

------

IDEA 3.

Con idea-2-3.py se generó un heatmap que muestra las correlaciones entre las distancias en el grafo y en los 
embeddings según el número y longitud de los caminos. La figura se guardó en la carpeta visualizations
con el nombre idea-3.pdf.

------

Se realizaron pequeñas modificaciones: 
+ cambio del tipo de cmap en el heatmap de correlaciones y anotación del valor de correlación (idea 3).
+ se usó "alpha" (grado de transparencia) en los scatterplots de la idea 2.
+ se arregló el gráfico de medias/std en el cual los puntos caían sobre la recta y = 1.

------

IDEA 4.

Para la misma red se cambió el valor de la semilla. Se fijó window-size = 3, representation-size = 2, number-walks = 5,
walk-lenght = 10, workers = 4.

Se usó --seed = [0, 3, 10, 25, 106, 500]

Ej.

deepwalk --format adjlist --input data/er-n30-p02.txt --max-memory-data-size 0 --number-walks 5 
-- representation-size 2 --walk-length 10 --window-size 3 --workers 4 --seed 0
--output deepwalk-embeddings/w3-x5-y10-seed0-er-n30-p02.txt

Los embeddings se guardaron en la carpeta deepwalk-embeddings.

Se los normalizó restando la media y dividiendo por la desviación estándar en cada dimensión. Los embeddings
normalizados se guardaron en la carpeta embeddings-normalized.

Asimismo, con idea-4.py se generó una única figura que contiene scatterplots de los
embeddings normalizados (con walk-lenght 10 y number-walks 5) cambiando sólo el valor de seed.

------

IDEA 5. (NO CONCRETADO AÚN)

Probamos graficar la distancia euclideana (embeddings) vs. otra propiedad de la red que no sean
los shortest paths entre los nodos. Ej. betweenness centrality.

------

IDEA 6.

Se generaron 3 nuevas redes de tipo Erdos-Renyi con igual p que la anterior (0.20) pero 
distinto número de nodos. Ej. N = {100, 1000}. Se guardaron las adjlists
correspondientes en la carpeta data.

---Red N = 100---

Se generaron 30 embeddings usando deepwalk variando el número de caminos (1, 5, 10, 25, 50 y 100)
y la longitud de los mismos (5, 10, 25, 50 y 100). Se fijó el tamaño de la vantana (window-size) en 3 y la
dimensión del embedding (representation-size) en 2.

Ej. de uso de deepwalk: (por default seed = 0)

deepwalk --format adjlist --input data/er-n30-p02.txt --max-memory-data-size 0 --number-walks 1 
-- representation-size 2 --walk-length 5 --window-size 3 --workers 4 
--output deepwalk-embeddings/w3-x1-y5-er-n30-p02.txt

Los embeddings se guardaban en achivos .txt dentro de la carpeta denominada deepwalk-embeddings.

Se normalizaron los embeddings obtenidos ejecutando el archivo normalize-embedding.py. Los nuevos embeddings
se guardaron en la carpeta embeddings-normalized.

A partir de los embeddings normalizados se generó el archivo idea-6-er-n30-p02.pdf en la carpeta visualizations
mediante figura.py con una única figura que contiene como subplots a los embeddings. Se puede apreciar la
disposición de los nodos a medida que se varían el número de caminos y su longitud. (Nota: no se etiquetaron a los
nodos por conveniencia, sin embargo, se hizo un gráfico similar etiquetando solo a algunos
idea-6-er-n100-p02-sample.pdf).

Se llevó a cabo la IDEA 1. (En relación a la normalización efectuada sobre los embeddings originales
nos preguntamos si exite algún efecto tras variar los valores de número y longitud de caminos).
Ver en la carpeta visualizations los archivos idea-1-mean-er-n100-p02.pdf, idea-1-std-er-n100-p02.pdf y
idea-1-mean-std-er-n100-p02.pdf

Se probó la IDEA 2 (relación entre los shortest path entre los nodos y la distancia euclideana de los mismos
en los embeddings). Ver en visualizations el archivo idea-2-er-n100-p02.pdf.

También se implementó la IDEA 3 (heatmap que muestra las correlaciones entre las distancias en el grafo y en los 
embeddings según el número y longitud de los caminos). Ver en visualizations el archivo idea-3-er-n100-p02.pdf.


---Red N = 1000---

Ídem red N = 100. (No se pudieron generar los gráficos de las "IDEA 2" por limitaciones de la PC.

------

Se creó la carpeta visualizations-resumen donde se colocó un resumen de algunas de las figuras generadas a la fecha.

------

+ Se creó el archivo dw.sh para agilizar y hacer más eficiente la generación de embeddings usando deepwalk.
+ Se cambió el tipo de archivo de imágenes de pdf a png.
+ A partir de idea-2-3.py se se creó idea-3.py (solo se separaron tareas).
+ Se verificaron las correlaciones. Nota: se está usando corrcoef() de Numpy la cual retorna la matriz
de covarianza normalizada.
+ Se generó una figura de los heatmaps de correlaciones de las redes N = 30, N = 100 y N = 1000. Se guardó en
la carpeta visualizations-resumen. (VER)
+ Se generó una figura con 5 repeticiones de un mismo embedding para la red erdos-renyi N = 1000 con seed = 0,
número de caminos 50 y longitud 50 (usando seed.sh y seed-0.py). Se guardaron los embeddings y los embeddings normalizados
correspondidentes (ej. w3-x50-y50-er-n1000-p02-seed0-1.txt). Se guardó la figura de los embeddings normalizados para
una misma semilla (0) en la carpeta visualizations (seeds0.png sin etiquetas en los nodos). Con el archivo
vis-seed-0.py se creó una figura seed-0-sample.png del mismo tipo pero con un sample sobre los nodos y se
los etiquetó.
+ Nuevamente se repitió el ítem anterior pero para valores de seeds diferentes (hay figuras de los embeddings
correspondientes completos y muestreados).

------

+ Se generó una red scale-free N=100, p=2.9 y m=5 con create-scale-free-py. Se obtuvo un archivo con una edgelist.

------

+ Se generaron embeddings variando el número de caminos y longitud de los mismos (los mismos valores
que para las redes erdos-renyi).
+ Se los normalizó y se aplicaron la IDEA-1 (media y desviación estándar),
IDEA-2 (distancia euclideana vs shortest path) e IDEA-3 (correlaciones). 
+ Se generó figura de embeddings según los parámetros y una similar haciendo un muestreo.

-----

+ Se hizo un ordenamiento del repo.


-----

Asunto SEEDS...

Dada una red, para un mismo valor de seed (ej. seed=0) en deepwalk se generan exactamente los mismos
caminos. Si uso distitos valores de seed se generan caminos diferentes. Esto fue comprobado al comparar
los corpus de paths que deepwalk produce.

Por lo tanto, la diferencia en los embeddings generados para un mismo seed no estaría provocada
por una aleatoriedad en la generación de los caminos sino en el Skipgram.

Más precisamente en la implementación de Word2Vec de Gensim...

En el docstring de Word2Vec, entre otras cosas, se especifica lo siguiente:
"""
seed : int, optional
            Seed for the random number generator. Initial vectors for each word are seeded with a hash of
            the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,
            you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter
            from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires
            use of the `PYTHONHASHSEED` environment variable to control hash randomization).
"""

Por lo tanto se generaron 2 embeddings para una misma red con igual seed (seed=0), fijando
workers=1 y usando un valor distinto de cero para la variable PYTHONHASHSEED.

En el archivo dw.sh para generar los embeddings se colocó:

export PYTHONHASHSEED=10

O bien, se puede hacer: $ PYTHONHASHSEED=10 python YOURSCRIPT.py

Especificar PYTHONHASHSEED=0 deshabilita el efecto.

Efectivamente, de esta manera, se logró que los embeddings generados en distintas ejecuciones
(para un mismo seed y valores de parámetros) sean iguales... :D

Fijar workers=1 hace que deepwalk trabaje muy lento...

-----

Se generaron embeddings para una red con parámetros fijos pero cambiando el valor de las semillas. Una tarea
que se realizó antes pero esta vez se implementó hacer workers=1 y PYTHONHASHSEED=10. Los embeddings igualmente
varían...

-----

Asuntos ha dejar de lado por ahora:

+ probar otros valores de windows-size
+ heatmap de correlaciones (shortest-paths y distancia euclideana)
+ figuras de mean, std y mean/std

------

Se vieron los videos sobre deepwalk:

https://slideplayer.com/slide/11064769/

https://neo4j.com/graphconnect-2018/session/deepwalk-graphs-network-embeddings-skiena

-----

Tareas que me restan hacer:
+ mirar mejor el paper
+ fase d>2 (distancia de Hamming)
+ ver algoritmos para linkprediction para implementar

-----

Un paquete para link prediction es "linkpred" v. 0.4.1 (pip install linkpred). EL mismo ofrece 
variedad de predictores. También muestra fácilmente cómo implementarlos.
También NetworkX tiene solo un par de algoritmos para link prediction pero sin ejemplos de uso.

Por simplicidad, voy a intentar usar "linkpred".

-----

+ Se agregaron los datasets de las ppi-networks en la carpeta data.
+ Upload de archivo red.py (num_nodos y num_edges)
+ Escritura de informe en inglés usando overleaf de lo que hay hasta ahora hecho.
+ Upload de archivo ppi-dw.sh (genera deepwalk embeddings de las ppi con valores de parámetros
  reportados en "Spectral Network Embedding: A Fast and Scalable Method via Sparsity" de Zhang et al. 2018 para
  cierta ppi. Específicamente windows-size=10, walk-length=40, representation-size=128 y walks=80.
+ Generación de embeddings de ejecuciones lentas... (pruebo con 2 ppi's: ratuss norvegicus e hybrid yeast).
+ Upload de embeddings sin normalizar y normalizados.
+ Uso de t-SNE para reducción de dimensionalidad para visualizar.
-----
+ Upload de archivo tsne-embedding.py donde se obtienen visualizaciones 2D tras usar la implementación
  de t-sne de scikit-learn.
+ Visualización de los embeddings generados de las dos redes ppi tras usar t-sne.
+ Uploads de visualizaciones en carpeta visualizations (tsne-rattus-d128.pdf y tsne-inter-d128.pdf).
-----
+ Se seleccionó una de las redes de ppi (rattus norvegicus) y se generaron más embeddings. Esta vez manteniendo
  los parámetros con valores windows-size=10, walk-length=40 y walks=80 (los usados en el paper mencionado
  anteriormente) pero cambiando el valor de representation-size (ej. 2 10 20 50 80 100).
+ Generación de una figura de los embeddings con diferentes dimenciones luego de usar t-sne con n-componets=2.
----

De la última reunión (martes 26/3/19) quedamos en lo siguiente:

+ Subir paper que mencioné para elegir parámetros de deepwalk al repo.
+ Subir figura de embeddings de la red inter_H-Y donde se varía el número de dimensiones (fijos los demás parámetros).
  Esta es una figura similar a la de rattus norvegicus y la idea era comparar la distribución de los nodos y clusters.
+ Probar con dimensión mayor a 128 en los embeddings.
+ Hacer un slice 2D de uno de los embeddings para ver cómo se ve compaarado con la versión del embedding usando t-SNE.
+ Sería interesante cambiar los valores de número de caminos y longitud de los mismos en altas
  dimensiones (no es prioritario).
+ Prioridades: 
            1) Aplicar un algoritmo de clustering directo sobre la red.
                         - sugerencias de Luis: k-means, DB-scan y Hierarchical Agglomerative Clustering.
                         - mirar link https://scikit-learn.org/stable/modules/clustering.html
            2) Comparar lo obtenido de 1) con lo obtenido tras aplicar deepwalk + t-SNE + clustering.
            3) Analizar y mostrar fracción de nodos que pertenecen a un cluster A en la dimensión X y que 
               siguen estando en el cluster A en la dimensión Y. (Pensar bien cómo mostrar la información).
               
-------

Se subió el paper: "Spectral Network Embedding: A Fast and Scalable Method via Sparsity" de Zhang et al. 2018 como
zhang-et-al-2018.pdf. Dentro de las cosas que se hizo allí fue aplicar deepwalk a una ppi (N=3,890 y E= 76,584)
con valores de parámetros por defecto (windows-size=10, walk-length=40, walks=80) pero con representation-size=128.
Hay otros (del estilo) que usan la misma red y aplican deepwalk con los mismos parámetros.

------

Se subió la figura de los embeddings de la red inter_H-Y (como la de rattus) con dimensiones (d): 2, 10, 20, 50,
80, 100, 128, 300 y 500. (Se probó con d > 128). Y se actualizó la de rattus sumando estas dos nuevos embeddings
de d=300 y d=500. Las figuras están guardadas en una nueva carperta: "ppi-visualizations".

------

